{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d4683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 1) (1000, 32, 32, 3) (1000, 1)\n",
      "val dataset accuracy: 0.091\n",
      "epoch: 1. loss: 2.3025733310664696\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 2. loss: 2.3023429192946843\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 3. loss: 2.3022167604995203\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 4. loss: 2.302135124729503\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 5. loss: 2.302076574056335\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 6. loss: 2.302037018449607\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 7. loss: 2.3020084807937833\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 8. loss: 2.301988187918079\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 9. loss: 2.301972618658357\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 10. loss: 2.301961078015659\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 11. loss: 2.301950043182075\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 12. loss: 2.3019399811331245\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 13. loss: 2.301932833563625\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 14. loss: 2.3019270361224526\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 15. loss: 2.301920958912521\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 16. loss: 2.3019158268260234\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 17. loss: 2.301911666453112\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 18. loss: 2.301906916017597\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 19. loss: 2.301903772539376\n",
      "val dataset accuracy: 0.099\n",
      "epoch: 20. loss: 2.3019009394142635\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, shape):\n",
    "        self.data = np.zeros(shape=shape, dtype=np.float32) # 存放数据\n",
    "        self.grad = np.zeros(shape=shape, dtype=np.float32) # 存放梯度\n",
    "\n",
    "    def clear_grad(self):\n",
    "        self.grad = np.zeros_like(self.grad)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Tensor shape: {}, data: {}\".format(self.data.shape, self.data)\n",
    "\n",
    "class Initializer:\n",
    "    \"\"\"\n",
    "    基类\n",
    "    \"\"\"\n",
    "    def __init__(self, shape=None, name='initializer'):\n",
    "        self.shape = shape\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class Constant(Initializer):\n",
    "    def __init__(self, value=0., name='constant initializer', *args, **kwargs):\n",
    "        super().__init__(name=name, *args, **kwargs)\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, shape=None, *args, **kwargs):\n",
    "        if shape:\n",
    "            self.shape = shape\n",
    "        assert shape is not None, \"the shape of initializer must not be None.\"\n",
    "        return self.value + np.zeros(shape=self.shape)\n",
    "\n",
    "\n",
    "class Normal(Initializer):\n",
    "    def __init__(self, mean=0., std=0.01, name='normal initializer', *args, **kwargs):\n",
    "        super().__init__(name=name, *args, **kwargs)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, shape=None, *args, **kwargs):\n",
    "        if shape:\n",
    "            self.shape = shape\n",
    "        assert shape is not None, \"the shape of initializer must not be None.\"\n",
    "        return np.random.normal(self.mean, self.std, size=self.shape)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raise NotImplementedError(\"'{}' not implement in class {}\"\n",
    "                                  .format('__getitem__', self.__class__.__name__))\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError(\"'{}' not implement in class {}\"\n",
    "                                  .format('__len__', self.__class__.__name__))\n",
    "\n",
    "class BatchSampler:\n",
    "    def __init__(self, dataset=None, shuffle=False, batch_size=1, drop_last=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.num_data = len(dataset)\n",
    "        if self.drop_last or (self.num_data % batch_size == 0):\n",
    "            self.num_samples = self.num_data // batch_size\n",
    "        else:\n",
    "            self.num_samples = self.num_data // batch_size + 1\n",
    "        indices = np.arange(self.num_data)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        if drop_last:\n",
    "            indices = indices[:self.num_samples * batch_size]\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_indices = []\n",
    "        for i in range(self.num_samples):\n",
    "            if (i + 1) * self.batch_size <= self.num_data:\n",
    "                for idx in range(i * self.batch_size, (i + 1) * self.batch_size):\n",
    "                    batch_indices.append(self.indices[idx])\n",
    "                yield batch_indices\n",
    "                batch_indices = []\n",
    "            else:\n",
    "                for idx in range(i * self.batch_size, self.num_data):\n",
    "                    batch_indices.append(self.indices[idx])\n",
    "        if not self.drop_last and len(batch_indices) > 0:\n",
    "            yield batch_indices\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, sampler=BatchSampler, shuffle=False, batch_size=1, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sampler = sampler\n",
    "        self.sampler = self.batch_sampler(dataset, shuffle, batch_size, drop_last)\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler)\n",
    "\n",
    "    def __call__(self):\n",
    "        self.__iter__()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample_indices in self.sampler:\n",
    "            data_list = []\n",
    "            label_list = []\n",
    "            for indice in sample_indices:\n",
    "                data, label = self.dataset[indice]\n",
    "                data_list.append(data)\n",
    "                label_list.append(label)\n",
    "            yield np.stack(data_list, axis=0), np.stack(label_list, axis=0)\n",
    "        self.sampler = self.batch_sampler(self.dataset, self.shuffle, self.batch_size, self.drop_last)\n",
    "        \n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx] / 255.0, self.Y[idx] # 图像值缩放到[0, 1]\n",
    "    \n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.val = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.val += value\n",
    "        self.count += n\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.val / self.count\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__call__())\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, name='layer', *args, **kwargs):\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        name='linear',\n",
    "        weight_attr=Normal(),\n",
    "        bias_attr=Constant(),\n",
    "        *args,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super().__init__(name=name, *args, **kwargs)\n",
    "        self.weights = Tensor((in_features, out_features))\n",
    "        self.weights.data = weight_attr(self.weights.data.shape)\n",
    "        self.bias = Tensor((1, out_features))\n",
    "        self.bias.data = bias_attr(self.bias.data.shape)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        output = np.dot(x, self.weights.data) + self.bias.data\n",
    "        return output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        self.weights.grad += np.dot(self.input.T, gradient)  # dy / dw\n",
    "        self.bias.grad += np.sum(gradient, axis=0, keepdims=True)  # dy / db \n",
    "        input_grad = np.dot(gradient, self.weights.data.T)  # dy / dx\n",
    "        return input_grad\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "\n",
    "    def __str__(self):\n",
    "        string = \"linear layer, weight shape: {}, bias shape: {}\".format(self.weights.data.shape, self.bias.data.shape)\n",
    "        return string\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, name='relu', *args, **kwargs):\n",
    "        super().__init__(name=name, *args, **kwargs)\n",
    "        self.activated = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[x < 0] = 0\n",
    "        self.activated = x\n",
    "        return self.activated\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        return gradient * (self.activated > 0)\n",
    "    \n",
    "class Sequential:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.graphs = []\n",
    "        self._parameters = []\n",
    "        for arg_layer in args:\n",
    "            if isinstance(arg_layer, Layer):\n",
    "                self.graphs.append(arg_layer)\n",
    "                self._parameters += arg_layer.parameters()\n",
    "\n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer), \"The type of added layer must be Layer, but got {}.\".format(type(layer))\n",
    "        self.graphs.append(layer)\n",
    "        self._parameters += layer.parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for graph in self.graphs:\n",
    "            x = graph(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # grad backward in inverse order of graph\n",
    "        for graph in self.graphs[::-1]:\n",
    "            grad = graph.backward(grad)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'Sequential:\\n'\n",
    "        for graph in self.graphs:\n",
    "            string += graph.__str__() + '\\n'\n",
    "        return string\n",
    "\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "    \n",
    "class Optimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.001, weight_decay=0.0, decay_type='l2'):\n",
    "        assert decay_type in ['l1', 'l2'], \"only support decay_type 'l1' and 'l2', but got {}.\".format(decay_type)\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.decay_type = decay_type\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.clear_grad()\n",
    "\n",
    "    def get_decay(self, g):\n",
    "        if self.decay_type == 'l1':\n",
    "            return self.weight_decay\n",
    "        elif self.decay_type == 'l2':\n",
    "            return self.weight_decay * g\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, momentum=0.9, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.momentum = momentum\n",
    "        self.velocity = []\n",
    "        for p in self.parameters:\n",
    "            self.velocity.append(np.zeros_like(p.grad))\n",
    "\n",
    "    def step(self):\n",
    "        for p, v in zip(self.parameters, self.velocity):\n",
    "            decay = self.get_decay(p.grad)\n",
    "            v = self.momentum * v + p.grad + decay \n",
    "            p.data = p.data - self.learning_rate * v\n",
    "            \n",
    "class SoftmaxWithLogits(Layer):\n",
    "    def __init__(self, reduction='mean', name='softamxwithlogits', *args, **kwargs):\n",
    "        super().__init__(name=name, *args, **kwargs)\n",
    "        assert reduction in ['mean', 'none', 'sum'], \"reduction only support 'mean', 'none' and 'sum', but got {}.\".format(reduction)\n",
    "        self.reduction = reduction\n",
    "        self.logits = None\n",
    "        self.target = None\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        assert logits.shape[0] == target.shape[0], \"The first fimension of logits and target is not same, logits shape {} cann't match target shape {}.\".format(logits.shape, target.shape)\n",
    "        self.logits = logits\n",
    "        self.target = target\n",
    "        loss = []\n",
    "        for i in range(logits.shape[0]):\n",
    "            loss_i = -logits[i, target.squeeze(-1)[i]] + np.log(np.sum(np.exp(logits[i])))\n",
    "            loss.append(loss_i)\n",
    "        loss = np.array(loss).reshape(target.shape)\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def backward(self):\n",
    "        soft_denominator = np.sum(np.exp(self.logits), axis=1, keepdims=True)  # [N, 1]\n",
    "        eq_grad = np.zeros_like(self.logits)\n",
    "        for i in range(self.logits.shape[0]):\n",
    "            eq_grad[i, self.target.squeeze(-1)[i]] = -1\n",
    "        gradient = np.exp(self.logits) / soft_denominator + eq_grad\n",
    "        return gradient\n",
    "\n",
    "epoches = 20   # epoch\n",
    "batch_size = 4  \n",
    "learning_rate = 0.001 \n",
    "num_classes = 10\n",
    "\n",
    "def load_cifar10(subset_percentage=10):\n",
    "    # Load the CIFAR-10 dataset\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "    # Select a random subset of the data\n",
    "    num_train_samples = int(len(train_images) * (subset_percentage / 100))\n",
    "    num_test_samples = int(len(test_images) * (subset_percentage / 100))\n",
    "\n",
    "    train_indices = np.random.choice(len(train_images), num_train_samples, replace=False)\n",
    "    test_indices = np.random.choice(len(test_images), num_test_samples, replace=False)\n",
    "\n",
    "    train_images = train_images[train_indices]\n",
    "    train_labels = train_labels[train_indices]\n",
    "    test_images = test_images[test_indices]\n",
    "    test_labels = test_labels[test_indices]\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    train_images = train_images.astype('float32') / 255.0\n",
    "    test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = load_cifar10()\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "train_dataset = CifarDataset(train_images, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = CifarDataset(test_images, test_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(3 * 32 * 32, 64, name='linear1'),\n",
    "    ReLU(name='relu1'),\n",
    "    Linear(64, 128, name='linear2'),\n",
    "    ReLU(name='relu1'),\n",
    "    Linear(128, 64, name='linear3'),\n",
    "    ReLU(name='relu1'),\n",
    "    Linear(64, num_classes, name='linear4'),\n",
    ")\n",
    "opt = SGD(parameters=model.parameters(), learning_rate=learning_rate, weight_decay=0.0, decay_type='l2')\n",
    "loss_fn = SoftmaxWithLogits()\n",
    "\n",
    "def eval(model, val_dataloader):\n",
    "    predict_labels = []\n",
    "    labels = []\n",
    "    for x, y in val_dataloader:\n",
    "        x = x.reshape((1, -1))\n",
    "        logits = model(x)\n",
    "        pred = np.argmax(logits, axis=1)\n",
    "        predict_labels.append(pred)\n",
    "        labels.append(y.squeeze(1))\n",
    "    pred = np.array(predict_labels)\n",
    "    labels = np.array(labels)\n",
    "    acc = np.sum(pred == labels) / len(labels)\n",
    "    print(\"val dataset accuracy:\", acc)\n",
    "    return acc\n",
    "\n",
    "total_acc = []  \n",
    "loss_avg = AverageMeter()\n",
    "for epoch in range(1, epoches + 1):\n",
    "    acc = eval(model, val_dataloader=val_dataloader)  \n",
    "    total_acc.append(acc)  \n",
    "    for idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.reshape((batch_size, -1))  \n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss_avg.update(loss)\n",
    "\n",
    "        grad = loss_fn.backward()\n",
    "        model.backward(grad)\n",
    "\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "    print(\"epoch: {}. loss: {}\".format(epoch, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db08cee2-ef7f-423e-88e7-0793156065f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/yhz2023/code_file/data/cifar-10-python.tar.gz\n",
      "Extracting /home/yhz2023/code_file/data/cifar-10-python.tar.gz to /home/yhz2023/code_file/data\n",
      "Files already downloaded and verified\n",
      "Epoch 1: Test Loss: 0.5240, Accuracy: 29.30%\n",
      "Epoch 2: Test Loss: 0.4879, Accuracy: 36.70%\n",
      "Epoch 3: Test Loss: 0.4657, Accuracy: 36.00%\n",
      "Epoch 4: Test Loss: 0.4532, Accuracy: 39.00%\n",
      "Epoch 5: Test Loss: 0.4404, Accuracy: 39.30%\n",
      "Epoch 6: Test Loss: 0.4353, Accuracy: 38.10%\n",
      "Epoch 7: Test Loss: 0.4219, Accuracy: 41.00%\n",
      "Epoch 8: Test Loss: 0.4146, Accuracy: 41.20%\n",
      "Epoch 9: Test Loss: 0.4115, Accuracy: 42.00%\n",
      "Epoch 10: Test Loss: 0.4050, Accuracy: 40.70%\n",
      "Epoch 11: Test Loss: 0.3985, Accuracy: 42.60%\n",
      "Epoch 12: Test Loss: 0.3933, Accuracy: 42.60%\n",
      "Epoch 13: Test Loss: 0.3864, Accuracy: 45.10%\n",
      "Epoch 14: Test Loss: 0.3820, Accuracy: 45.30%\n",
      "Epoch 15: Test Loss: 0.3786, Accuracy: 44.80%\n",
      "Epoch 16: Test Loss: 0.3736, Accuracy: 46.30%\n",
      "Epoch 17: Test Loss: 0.3709, Accuracy: 46.60%\n",
      "Epoch 18: Test Loss: 0.3667, Accuracy: 46.30%\n",
      "Epoch 19: Test Loss: 0.3650, Accuracy: 46.30%\n",
      "Epoch 20: Test Loss: 0.3646, Accuracy: 46.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 32 * 16 * 16)  # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def load_cifar10(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(root='/home/yhz2023/code_file/data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='/home/yhz2023/code_file/data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Loading smaller subset of CIFAR-10\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, torch.arange(0, 5000))  # 10% of 50000\n",
    "    test_subset = torch.utils.data.Subset(test_dataset, torch.arange(0, 1000))  # 10% of 10000\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001)  # L2 regularization as weight_decay\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}: Test Loss: {test_loss:.4f}, Accuracy: {100. * correct / len(test_loader.dataset):.2f}%')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, test_loader = load_cifar10(batch_size=4)\n",
    "    model = CNNModel()\n",
    "    train_model(model, train_loader, test_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e9071-2ea6-4968-8cc5-b1f5d8329108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
