{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78c95e8-d425-4d37-be01-49b985c48727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 1) (1000, 32, 32, 3) (1000, 1)\n",
      "Epoch 1\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 2\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 3\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 4\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 5\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 6\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 7\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 8\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 9\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 10\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 11\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 12\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 13\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 14\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n",
      "Epoch 15\n",
      "Loss = inf | Training Accuracy = 0.1072 | Test Accuracy = 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def load_cifar10(subset_percentage=10):\n",
    "    # Load the CIFAR-10 dataset\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "    # Select a random subset of the data\n",
    "    num_train_samples = int(len(train_images) * (subset_percentage / 100))\n",
    "    num_test_samples = int(len(test_images) * (subset_percentage / 100))\n",
    "\n",
    "    train_indices = np.random.choice(len(train_images), num_train_samples, replace=False)\n",
    "    test_indices = np.random.choice(len(test_images), num_test_samples, replace=False)\n",
    "\n",
    "    train_images = train_images[train_indices]\n",
    "    train_labels = train_labels[train_indices]\n",
    "    test_images = test_images[test_indices]\n",
    "    test_labels = test_labels[test_indices]\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    train_images = train_images.astype('float32') / 255.0\n",
    "    test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = load_cifar10()\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "from layers import *\n",
    "from solver import sgd, sgd_momentum, adam\n",
    "from loss import SoftmaxLoss, l2_regularization, delta_l2_regularization\n",
    "from utils import softmax\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, layers, loss_func=SoftmaxLoss):\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params.append(layer.params)\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = []\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss, dout = self.loss_func(out, y)\n",
    "        loss += l2_regularization(self.layers)\n",
    "        grads = self.backward(dout)\n",
    "        grads = delta_l2_regularization(self.layers, grads)\n",
    "        return loss, grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(softmax(X), axis=1)\n",
    "    \n",
    "    \n",
    "    def accuracy(y_true, y_pred):\n",
    "        return np.mean(y_pred == y_true) \n",
    "\n",
    "def make_cnn(X_dim, num_class):\n",
    "    conv = Conv(X_dim, n_filter=32, h_filter=3,\n",
    "                w_filter=3, stride=1, padding=1)\n",
    "    relu_conv = ReLU()\n",
    "    maxpool = Maxpool(conv.out_dim, size=2, stride=1)\n",
    "    flat = Flatten()\n",
    "    fc = FullyConnected(np.prod(maxpool.out_dim), num_class)\n",
    "    return [conv, relu_conv, maxpool, flat, fc]\n",
    "\n",
    "\n",
    "cifar_dims = (32, 32, 3)\n",
    "cnn = CNN(make_cnn(cifar_dims, num_class=10))\n",
    "cnn = sgd_momentum(cnn, train_images, train_labels, minibatch_size=256, epoch=15,\n",
    "                   learning_rate=0.01, X_test=test_images, y_test=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db08cee2-ef7f-423e-88e7-0793156065f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac5dc56e8414a189a1b964bdb0a29b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch 1: Test Loss: 0.0365, Accuracy: 19.90%\n",
      "Epoch 2: Test Loss: 0.0363, Accuracy: 22.90%\n",
      "Epoch 3: Test Loss: 0.0362, Accuracy: 23.80%\n",
      "Epoch 4: Test Loss: 0.0361, Accuracy: 24.40%\n",
      "Epoch 5: Test Loss: 0.0360, Accuracy: 25.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 32 * 16 * 16)  # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def load_cifar10(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Loading smaller subset of CIFAR-10\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, torch.arange(0, 5000))  # 10% of 50000\n",
    "    test_subset = torch.utils.data.Subset(test_dataset, torch.arange(0, 1000))  # 10% of 10000\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001)  # L2 regularization as weight_decay\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Manual L1 regularization\n",
    "            l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss += 0.01 * l1_penalty  # Apply L1 regularization\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}: Test Loss: {test_loss:.4f}, Accuracy: {100. * correct / len(test_loader.dataset):.2f}%')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, test_loader = load_cifar10(batch_size=64)\n",
    "    model = CNNModel()\n",
    "    train_model(model, train_loader, test_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e9071-2ea6-4968-8cc5-b1f5d8329108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
