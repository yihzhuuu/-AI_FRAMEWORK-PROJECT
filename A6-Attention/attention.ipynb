{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f232e12a-58dc-4b4b-88fd-30fd5c196dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with GRU RNN and bahdanau attention...\n",
      "Epoch: 1, Train Loss: 4.340084726064741, Valid Loss: 3.65247443318367\n",
      "Epoch: 2, Train Loss: 3.1268660707095646, Valid Loss: 3.295613706111908\n",
      "Epoch: 3, Train Loss: 2.675323183841117, Valid Loss: 3.2827012836933136\n",
      "Epoch: 4, Train Loss: 2.374042877541765, Valid Loss: 3.2510056495666504\n",
      "Epoch: 5, Train Loss: 2.153458273358282, Valid Loss: 3.2489570677280426\n",
      "Epoch: 6, Train Loss: 2.0075077480156516, Valid Loss: 3.2760799527168274\n",
      "Epoch: 7, Train Loss: 1.9041924266563113, Valid Loss: 3.23938250541687\n",
      "Epoch: 8, Train Loss: 1.7949174437753954, Valid Loss: 3.299146056175232\n",
      "Epoch: 9, Train Loss: 1.7038813587852513, Valid Loss: 3.3199033737182617\n",
      "Epoch: 10, Train Loss: 1.6570884798066732, Valid Loss: 3.4353507459163666\n",
      "Test Loss for GRU + bahdanau: 3.431431472301483\n",
      "BLEU Score for GRU + bahdanau: 0.24\n",
      "Training with GRU RNN and luong attention...\n",
      "Epoch: 1, Train Loss: 4.621248229484726, Valid Loss: 4.406205058097839\n",
      "Epoch: 2, Train Loss: 3.7314060837161698, Valid Loss: 3.9374460875988007\n",
      "Epoch: 3, Train Loss: 3.365824588069832, Valid Loss: 3.7247065007686615\n",
      "Epoch: 4, Train Loss: 3.066442353084749, Valid Loss: 3.6006547808647156\n",
      "Epoch: 5, Train Loss: 2.8910476930341007, Valid Loss: 3.508498191833496\n",
      "Epoch: 6, Train Loss: 2.776020268511667, Valid Loss: 3.5677094757556915\n",
      "Epoch: 7, Train Loss: 2.621378478499761, Valid Loss: 3.5634090900421143\n",
      "Epoch: 8, Train Loss: 2.608216255246805, Valid Loss: 3.635767787694931\n",
      "Epoch: 9, Train Loss: 2.5907325156459726, Valid Loss: 3.632890373468399\n",
      "Epoch: 10, Train Loss: 2.4378923277497817, Valid Loss: 3.5175822377204895\n",
      "Test Loss for GRU + luong: 3.5577057600021362\n",
      "BLEU Score for GRU + luong: 0.18\n",
      "Training with LSTM RNN and bahdanau attention...\n",
      "Epoch: 1, Train Loss: 4.455091648689976, Valid Loss: 3.867539644241333\n",
      "Epoch: 2, Train Loss: 3.2882622958279915, Valid Loss: 3.422882854938507\n",
      "Epoch: 3, Train Loss: 2.7828233168513763, Valid Loss: 3.3321017920970917\n",
      "Epoch: 4, Train Loss: 2.492257053106367, Valid Loss: 3.241404414176941\n",
      "Epoch: 5, Train Loss: 2.2687002987588554, Valid Loss: 3.274786114692688\n",
      "Epoch: 6, Train Loss: 2.0945818240421983, Valid Loss: 3.3416822254657745\n",
      "Epoch: 7, Train Loss: 1.9729069103753514, Valid Loss: 3.291298031806946\n",
      "Epoch: 8, Train Loss: 1.8588366114620596, Valid Loss: 3.3159656822681427\n",
      "Epoch: 9, Train Loss: 1.7604265023958316, Valid Loss: 3.374606430530548\n",
      "Epoch: 10, Train Loss: 1.6556500658589839, Valid Loss: 3.3793950378894806\n",
      "Test Loss for LSTM + bahdanau: 3.416946619749069\n",
      "BLEU Score for LSTM + bahdanau: 0.24\n",
      "Training with LSTM RNN and luong attention...\n",
      "Epoch: 1, Train Loss: 4.569916922615488, Valid Loss: 4.244341641664505\n",
      "Epoch: 2, Train Loss: 3.658350879400312, Valid Loss: 3.9367616176605225\n",
      "Epoch: 3, Train Loss: 3.247554754895786, Valid Loss: 3.7825777530670166\n",
      "Epoch: 4, Train Loss: 3.0454465607714547, Valid Loss: 3.690885156393051\n",
      "Epoch: 5, Train Loss: 2.8558740626347747, Valid Loss: 3.7000407576560974\n",
      "Epoch: 6, Train Loss: 2.705958347488605, Valid Loss: 3.566389501094818\n",
      "Epoch: 7, Train Loss: 2.5723166171674685, Valid Loss: 3.613022744655609\n",
      "Epoch: 8, Train Loss: 2.4902244811541183, Valid Loss: 3.5584298968315125\n",
      "Epoch: 9, Train Loss: 2.5070745734916384, Valid Loss: 3.6123784482479095\n",
      "Epoch: 10, Train Loss: 2.3596630469292794, Valid Loss: 3.579606682062149\n",
      "Test Loss for LSTM + luong: 3.5898908376693726\n",
      "BLEU Score for LSTM + luong: 0.19\n",
      "RNN Type: GRU, Attention Type: bahdanau, Test Loss: 3.431431472301483\n",
      "RNN Type: GRU, Attention Type: luong, Test Loss: 3.5577057600021362\n",
      "RNN Type: LSTM, Attention Type: bahdanau, Test Loss: 3.416946619749069\n",
      "RNN Type: LSTM, Attention Type: luong, Test Loss: 3.5898908376693726\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "data_folder = \"/home/yhz2023/code_file/data\"\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG), root=data_folder)\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn_type = rnn_type\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]  # LSTM returns hidden state and cell state, we take hidden state\n",
    "        s = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        return outputs, s\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, s, enc_output):\n",
    "        batch_size = enc_output.shape[1]\n",
    "        src_len = enc_output.shape[0]\n",
    "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        enc_output = enc_output.transpose(0, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(dec_hid_dim, enc_hid_dim * 2) \n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch size, dec_hid_dim]\n",
    "        hidden = self.attn(hidden) \n",
    "        hidden = hidden.unsqueeze(2)  # [batch size, enc_hid_dim * 2, 1]\n",
    "\n",
    "        # encoder_outputs: [src_len, batch size, enc_hid_dim * 2]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch size, enc_hid_dim * 2, src_len]\n",
    "        energy = torch.bmm(encoder_outputs, hidden).squeeze(2)  # [batch size, src_len]\n",
    "\n",
    "        return F.softmax(energy, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, dec_input, s, enc_output):\n",
    "        dec_input = dec_input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(dec_input)).transpose(0, 1)\n",
    "        a = self.attention(s, enc_output).unsqueeze(1)\n",
    "        enc_output = enc_output.transpose(0, 1)\n",
    "        c = torch.bmm(a, enc_output).transpose(0, 1)\n",
    "        rnn_input = torch.cat((embedded, c), dim = 2)\n",
    "        dec_output, dec_hidden = self.rnn(rnn_input, s.unsqueeze(0))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        dec_output = dec_output.squeeze(0)\n",
    "        c = c.squeeze(0)\n",
    "        pred = self.fc_out(torch.cat((dec_output, c, embedded), dim = 1))\n",
    "        \n",
    "        return pred, dec_hidden.squeeze(0)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "       \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        enc_output, s = self.encoder(src)\n",
    "        dec_input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            dec_output, s = self.decoder(dec_input, s, enc_output)\n",
    "            outputs[t] = dec_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = dec_output.argmax(1) \n",
    "            dec_input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "def initialize_model(rnn_type, attn_type, input_dim, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "    encoder = Encoder(input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, rnn_type=rnn_type)\n",
    "    \n",
    "    if attn_type == 'bahdanau':\n",
    "        attention = BahdanauAttention(enc_hid_dim, dec_hid_dim)\n",
    "    elif attn_type == 'luong':\n",
    "        attention = LuongAttention(enc_hid_dim, dec_hid_dim)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported attention type\")\n",
    "\n",
    "    decoder = Decoder(output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention)\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, train_iterator, valid_iterator, optimizer, criterion, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            src, trg = batch.src, batch.trg\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss / len(train_iterator)}, Valid Loss: {valid_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch.src, batch.trg\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def translate(model, iterator, trg_field):\n",
    "    model.eval()\n",
    "    original_texts = []\n",
    "    generated_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src.to(device)\n",
    "            trg = batch.trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            output = output.argmax(2).transpose(0, 1).tolist()\n",
    "\n",
    "            # Convert indexes to strings\n",
    "            for j in range(trg.shape[1]):\n",
    "                original_text = [trg_field.vocab.itos[token] for token in trg[:, j] if token != trg_field.vocab.stoi['<pad>']]\n",
    "                generated_text = [trg_field.vocab.itos[token] for token in output[j] if token != trg_field.vocab.stoi['<pad>']]\n",
    "\n",
    "                original_texts.append([original_text])\n",
    "                generated_texts.append(generated_text)\n",
    "\n",
    "    return original_texts, generated_texts\n",
    "\n",
    "def calculate_bleu(original_texts, generated_texts):\n",
    "    return corpus_bleu(original_texts, generated_texts)\n",
    "\n",
    "configurations = [\n",
    "    (\"GRU\", \"bahdanau\"),\n",
    "    (\"GRU\", \"luong\"),\n",
    "    (\"LSTM\", \"bahdanau\"),\n",
    "    (\"LSTM\", \"luong\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for rnn_type, attn_type in configurations:\n",
    "    model = initialize_model(rnn_type, attn_type, INPUT_DIM, OUTPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(f'Training with {rnn_type} RNN and {attn_type} attention...')\n",
    "    train_and_evaluate(model, train_iterator, valid_iterator, optimizer, criterion, n_epochs=10)\n",
    "    \n",
    "    test_loss = evaluate(model, test_iterator, criterion)\n",
    "    print(f'Test Loss for {rnn_type} + {attn_type}: {test_loss}')\n",
    "    results.append((rnn_type, attn_type, test_loss))\n",
    "\n",
    "    original_texts, generated_texts = translate(model, test_iterator, TRG)\n",
    "    bleu_score = calculate_bleu(original_texts, generated_texts)\n",
    "    print(f'BLEU Score for {rnn_type} + {attn_type}: {bleu_score:.2f}')\n",
    "\n",
    "for result in results:\n",
    "    print(f\"RNN Type: {result[0]}, Attention Type: {result[1]}, Test Loss: {result[2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
